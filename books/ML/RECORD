#!/usr/bin/env python
ciml

CHAPTER 1 ( Decision Trees )

Section 1.1 ( What Does it Mean to Learn )
# what is learning?
	generlize rather than memorize
# training data | test set
# Goal: 
	the machine learning algorithm has succeeded if its performance on the test data is high 

Section 1.2 ( Some Canonical Learning Problems )
# Regression ( 回归 )
	trying to predict a real value.
# Binary Classification ( 二叉 )
	trying to predict a simple yes/no response.
# Multiclass Classification ( 聚类 )
	trying to put an example into one of a number of classes.
# Ranking ( web 3.0 )
	trying to put a set of objects in order of relevance.

Section 1.3 ( The Desicion Tree Model of Learning )
# The goal is to figure out:
	* what question to ask
	* in what order
	* what answer to predict
# Concepts
	* features, i.e. the questions
	* feature values, responses to questions, tuples
	* label, the rating
	* training data contains a set of feature values paired with labels
	* an example is a set of feature values
	   the algorithm is expected to predict the label
# What order?
	decision tree is builded greedily. 
	--( what would you ask if only one question is permitted )
# how well would you have done?
	score the question
# the algorithm
	recurse greedily

Section 1.4 ( Formalizing the Learning Problem )
# loss function ( Score )
	* Regression:
		squared loss --> l(y,ŷ) = (y-ŷ)²
		absolute loss --> l(y,ŷ) = |y-ŷ|
	* Binary Classification:
		zero/one loss --> l(y,ŷ) = 0 if y=ŷ otherwise = 1
	* Multiclass Classification:
		zero/one loss
# Probabilistic model ( Data )
	Probability distribution D(x,y) over input x and output y

Section 1.5 ( Inductive Bias: What We Know Before the Data Arrives )
	in the absense of data that narrow down the relevant concept, what type of solutions are we more likely to prefer?

Section 1.6 ( Not Everything is Learnable )
# noise
	"noise" in training data: at feature level or at label level

Section 1.7 ( Underfitting and Overfitting )
	This Concept is Brilliant!
	learning too little or too complicated both sucks

Section 1.8 ( Separation of Training and Test Data )
# Never ever touch your test data!

Section 1.9 ( Models, Parameters and Hyperparameters )
# Hyperparameter:
	hyperparameter is so called because it controls other parameters of the model.
	training data, development data, test data

Section 1.10 ( Chapter Summary and Outlook )
# summary:
	model and associated inductive biases
	training data --> parameters
	development data --> avoid underfitting and overfitting
	test data --> estimate future model performance

Section 1.11 ( Exercises )

CHAPTER 2 ( Geometry and Nearest Neighbors )
